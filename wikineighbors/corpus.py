from cached_property import cached_property
import numpy as np
from collections import Counter
from sklearn.metrics.pairwise import cosine_similarity
from timeit import default_timer as timer
import pickle
import spacy
from wikineighbors.utils import regex_digit

from wikineighbors.params import Params
from wikineighbors.exceptions import WikiNeighborsNoArticlesFound
from wikineighbors.exceptions import WikiNeighborsNoVocabFound
from wikineighbors.utils import gen_100_param_names, to_param_path
from wikineighbors import config

nlp = spacy.load('en_core_web_sm')


class Corpus:
    """
    an abstraction.
    contains info about text file locations on shared drive
    """

    def __init__(self, name):
        self.name = name
        self.cache_path = config.LocalDirs.cache / name
        self.vocab_file_name_template = 'vocab_{}.pkl'
        self.sim_mat_file_name_template = 'sim_mat_{}.pkl'
        if not self.cache_path.is_dir():
            self.cache_path.mkdir(parents=True)

    # ------------------------------------------------ basic info

    @cached_property
    def num_param_names(self):
        return len(self.param_names)

    @cached_property
    def param_names(self):
        param_names = []
        parts = set()
        for param_name in gen_100_param_names(self.name):
            param_path = to_param_path(param_name)

            # make sure path exists
            if not param_path.exists():
                break  # this break is required, else infinite loop

            # make sure that param_name is valid (that it really is part of the corpus)
            params = Params(param_path)
            if params.part in parts:
                continue  # this happens because param_names are generated by returning all 8 possible param_names

            # collect
            parts.add(params.part)
            param_names.append(param_name)
        return param_names

    @cached_property
    def txt_paths(self):
        res = []
        for param_name in self.param_names:
            param_path = to_param_path(param_name)
            # check that articles are available (bodies.txt files)
            body_paths = [p for p in param_path.glob('**/bodies.txt')]
            num_bodies = len(body_paths)
            if num_bodies == 0:
                raise WikiNeighborsNoArticlesFound(param_path)
            elif num_bodies > 1:
                raise SystemError('Found more than 1 bodies.txt files in {}'.format(param_name))
            else:
                res.append(body_paths[0])

        return res

    # --------------------------------------------------- counting words

    def gen_docs(self):
        for p in self.txt_paths:
            with p.open('r') as f:  # it takes 8 seconds no matter how many processes due to network read
                for doc in f:  # lazy generator
                    yield doc

    def make_counts(self):
        print('Counting all words in {} docs...'.format(config.Max.num_docs))
        start = timer()
        w2cf = Counter()
        w2dfs = []
        n = 0
        for doc in nlp.pipe(self.gen_docs(),
                            batch_size=config.Corpus.batch_size,
                            disable=['tagger', 'parser', 'ner']):
            words = [w.text for w in doc]
            if not words:
                print('WARNING: No words found')
                continue

            w2df = Counter(words)
            w2dfs.append(w2df)
            w2cf.update(w2df)  # frequencies are incremented rather than replaced

            n += 1
            if n == config.Max.num_docs:
                break

        print('Took {} secs to count all words in {} docs in 1 process'.format(
            timer() - start, config.Max.num_docs))
        return w2cf, w2dfs

    # ------------------------------------------------------------ neighbors

    @cached_property
    def w2id(self):
        return {w: i for i, w in enumerate(self.vocab)}

    def get_neighbors(self, word):
        if not self.cached_vocab_sizes:
            raise WikiNeighborsNoVocabFound(self.name)

        print('Computing neighbors...')
        sims = self.sim_mat[self.w2id[word]]
        res = [(w, s) for w, s in sorted(zip(self.vocab, sims), key=lambda i: i[1])
               if w != word]
        return zip(*res)  # unpack [(w, s), ...(w, s)] to [w, ...w], [s, ..., s]

    # --------------------------------------------------- i/o

    def save_to_disk(self, size):
        # make vocab + sim mat
        w2cf, w2dfs = self.make_counts()
        vocab = self.make_vocab(w2cf, size)

        assert len(w2dfs) == config.Max.num_docs
        if not len(vocab) == config.Max.num_words:
            raise RuntimeError('Did not find enough vocab words.'
                               ' Try setting number of documents higher')
        sim_mat = self.make_sim_mat(w2dfs, vocab)

        with (self.cache_path / self.vocab_file_name_template.format(size)).open('wb') as f:
            pickle.dump(vocab, f)
        print('Saved vocab to {}'.format(config.LocalDirs.cache))

        with (self.cache_path / self.sim_mat_file_name_template.format(size)).open('wb') as f:
            pickle.dump(sim_mat, f)
        print('Saved sim_mat to {}'.format(config.LocalDirs.cache))

    # ----------------------------------------------------- vocab

    @staticmethod
    def make_vocab(w2cf, size):
        print('Making vocab...')
        return [w for w, f in sorted(w2cf.most_common(size))]

    @cached_property
    def cached_vocab_sizes(self):
        sizes = []
        for p in self.cache_path.glob('vocab_*.pkl'):
            size = int(regex_digit.search(p.name).group())
            sizes.append(size)
        return sorted(sizes)

    def load_largest_vocab(self):
        largest_size = self.cached_vocab_sizes[-1]
        file_name = self.vocab_file_name_template.format(largest_size)
        with (self.cache_path / file_name).open('rb') as f:
            res = pickle.load(f)
        assert len(res) == largest_size
        return res

    @cached_property
    def vocab(self):
        if not self.cached_vocab_sizes:
            raise WikiNeighborsNoVocabFound(self.name)
        else:
            return self.load_largest_vocab()

    # --------------------------------------------------- sim mat

    @staticmethod
    def make_sim_mat(w2dfs, vocab):  # TODO this may benefit from multiprocessing
        print('Making term-by-doc matrix...')
        term_by_doc_mat = np.zeros((config.Max.num_words, config.Max.num_docs))
        for col_id, w2df in enumerate(w2dfs):
            try:
                term_by_doc_mat[:, col_id] = [w2df[w] for w in vocab]
            except IndexError:
                print(col_id)
        print('Done')
        return cosine_similarity(term_by_doc_mat)

    def load_largest_sim_mat(self):
        largest_size = self.cached_vocab_sizes[-1]
        file_name = self.sim_mat_file_name_template.format(largest_size)
        with (self.cache_path / file_name).open('rb') as f:
            res = pickle.load(f)
        assert len(res) == largest_size
        return res

    @cached_property
    def sim_mat(self):
        if not self.cached_vocab_sizes:
            raise WikiNeighborsNoVocabFound(self.name)
        else:
            return self.load_largest_sim_mat()