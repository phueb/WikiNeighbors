from cached_property import cached_property
import numpy as np
from collections import Counter
from stop_words import get_stop_words
from sklearn.metrics.pairwise import cosine_similarity

from wikineighbors.io import Params
from wikineighbors.exceptions import LudwigVizNoArticlesFound
from wikineighbors.utils import gen_100_param_names, to_param_path
from wikineighbors import config


stop_words = get_stop_words('en')


class Corpus:
    """
    an abstraction.
    contains info about text file locations on shared drive
    """

    def __init__(self, name, vocab=None):
        self.name = name
        self.param_names = self.get_param_names()
        self._vocab = vocab

    @cached_property
    def num_param_names(self):
        return len(self.param_names)

    def get_param_names(self):
        param_names = []
        parts = set()
        for param_name in gen_100_param_names(self.name):
            param_path = to_param_path(param_name)

            # make sure path exists
            if not param_path.exists():
                break  # this break is required, else infinite loop

            # make sure that param_name is valid (that it really is part of the corpus)
            params = Params(param_path)
            if params.part in parts:
                continue  # this happens because param_names are generated by returning all 8 possible param_names

            # collect
            parts.add(params.part)
            param_names.append(param_name)
        return param_names

    @cached_property
    def txt_paths(self):
        res = []
        for param_name in self.param_names:
            param_path = to_param_path(param_name)
            # check that articles are available (bodies.txt files)
            body_paths = [p for p in param_path.glob('**/bodies.txt')]
            num_bodies = len(body_paths)
            if num_bodies == 0:
                raise LudwigVizNoArticlesFound(param_path)
            elif num_bodies > 1:
                raise SystemError('Found more than 1 bodies.txt files in {}'.format(param_name))
            else:
                res.append(body_paths[0])

        return res

    def gen_docs(self):
        for p in self.txt_paths:
            with p.open('r') as f:
                for doc in f.readlines():
                    yield doc

    @cached_property
    def w2dfs(self):

        # TODO a classic map-reduce problem:
        #  put a queue in a different process generating docs, and have a set of worker processes operate on them

        w2dfs = []
        num_docs = 0
        for doc in self.gen_docs():
            words = [w for w in doc.split() if w not in stop_words]
            w2df = Counter(words)
            w2dfs.append(w2df)
            num_docs += 1
            if num_docs == config.Max.num_docs:
                break
        return w2dfs

    @cached_property
    def w2f(self):
        res = Counter()
        for w2df in self.w2dfs:
            res.update(w2df)  # frequencies are incremented rather than replaced
        return res

    @cached_property
    def w2id(self):
        return {w: i for i, w in enumerate(self.vocab)}

    @cached_property
    def vocab(self):
        if self._vocab:
            return self._vocab
        else:
            return [w for w, f in sorted(self.w2f.most_common(config.Max.num_words))]

    @cached_property
    def term_by_doc_mat(self):
        res = np.zeros((config.Max.num_words, config.Max.num_docs))
        for col_id, w2df in enumerate(self.w2dfs):
            res[:, col_id] = [w2df[w] for w in self.vocab]
        return res

    @cached_property
    def sim_mat(self):
        return cosine_similarity(self.term_by_doc_mat)