from cached_property import cached_property
import numpy as np
from collections import Counter
from sklearn.metrics.pairwise import cosine_similarity
from timeit import default_timer as timer
import pickle
import spacy
from wikineighbors.utils import regex_digit

from wikineighbors.params import Params
from wikineighbors.exceptions import WikiNeighborsNoArticlesFound
from wikineighbors.exceptions import WikiNeighborsNoMemory
from wikineighbors.exceptions import WikiNeighborsNoVocabFound
from wikineighbors.utils import gen_100_param_names, to_param_path
from wikineighbors import config

nlp = spacy.load('en_core_web_sm')


class Corpus:
    """
    an abstraction.
    contains info about text file locations on shared drive
    """

    def __init__(self, name):
        self.name = name
        self.cache_path = config.LocalDirs.cache / name
        self.vocab_file_name_template = 'vocab_{}_{}.pkl'
        self.sim_mat_file_name_template = 'sim_mat_{}_{}.pkl'
        if not self.cache_path.is_dir():
            self.cache_path.mkdir(parents=True)

    # ------------------------------------------------ basic info

    @cached_property
    def num_param_names(self):
        return len(self.param_names)

    @cached_property
    def param_names(self):
        param_names = []
        parts = set()
        for param_name in gen_100_param_names(self.name):
            param_path = to_param_path(param_name)

            # make sure path exists
            if not param_path.exists():
                break  # this break is required, else infinite loop

            # make sure that param_name is valid (that it really is part of the corpus)
            params = Params(param_path)
            if params.part in parts:
                continue  # this happens because param_names are generated by returning all 8 possible param_names

            # collect
            parts.add(params.part)
            param_names.append(param_name)
        return param_names

    @cached_property
    def txt_paths(self):
        res = []
        for param_name in self.param_names:
            param_path = to_param_path(param_name)
            # check that articles are available (bodies.txt files)
            body_paths = [p for p in param_path.glob('**/bodies.txt')]
            num_bodies = len(body_paths)
            if num_bodies == 0:
                raise WikiNeighborsNoArticlesFound(param_path)
            elif num_bodies > 1:
                raise SystemError('Found more than 1 bodies.txt files in {}'.format(param_name))
            else:
                res.append(body_paths[0])

        return res

    # --------------------------------------------------- counting words

    def gen_docs(self):
        for p in self.txt_paths:
            with p.open('r') as f:  # it takes 8 seconds no matter how many processes due to network read
                for doc in f:  # lazy generator
                    yield doc

    def make_counts(self):
        print('Counting all words in {} docs...'.format(config.Max.num_docs))
        start = timer()
        w2cf = Counter()
        w2dfs = []
        n = 0

        # generating docs is very fast, but spacy tokenization is very slow
        for doc in nlp.pipe(self.gen_docs(),
                            batch_size=config.Corpus.batch_size,
                            disable=['tagger', 'parser', 'ner']):
            words = [w.lemma_ for w in doc]
            if not words:
                print('WARNING: No words found')
                continue

            w2df = Counter(words)  # this is very fast
            w2dfs.append(w2df)
            w2cf.update(w2df)  # frequencies are incremented rather than replaced

            n += 1
            if n == config.Max.num_docs:
                break

            if n % 1000 == 0:
                print(n)

        print('Took {} secs to count all words in {} docs'.format(
            timer() - start, config.Max.num_docs))
        return w2cf, w2dfs

    # ------------------------------------------------------------ neighbors

    @cached_property
    def w2id(self):
        return {w: i for i, w in enumerate(self.vocab)}

    def get_neighbors(self, word):
        if not self.cached_vocab_sizes:
            raise WikiNeighborsNoVocabFound(self.name)

        print('Computing neighbors...')
        sims = self.sim_mat[self.w2id[word]]
        res = [(w, s) for w, s in sorted(zip(self.vocab, sims), key=lambda i: i[1])
               if w != word]
        return zip(*res)  # unpack [(w, s), ...(w, s)] to [w, ...w], [s, ..., s]

    # --------------------------------------------------- i/o

    def save_to_disk(self, size, cat):
        """
        make vocab + save to disk.
        if cats is not None, exclude all words from vocab that are not in cats (part-of-speech categories)
        """

        # check that memory is sufficient
        shape = (size, config.Max.num_docs)
        try:
            init_mat = np.zeros(shape, dtype=np.int16)
            num_mbs = init_mat.nbytes / 1e6
            print('Successfully initialized matrix with shape={} requiring {} megabytes'.format(shape, num_mbs))
        except MemoryError:
            raise WikiNeighborsNoMemory(shape)

        # make vocab + sim mat
        print('Making vocab with size={} and cat={}'.format(size, cat))
        w2cf, w2dfs = self.make_counts()
        vocab = self.make_vocab(w2cf, size, cat)

        assert len(w2dfs) == config.Max.num_docs
        sim_mat = self.make_sim_mat(w2dfs, vocab, init_mat)

        with (self.cache_path / self.vocab_file_name_template.format(size, cat)).open('wb') as f:
            pickle.dump(vocab, f)
        print('Saved vocab to {}'.format(config.LocalDirs.cache))

        with (self.cache_path / self.sim_mat_file_name_template.format(size, cat)).open('wb') as f:
            pickle.dump(sim_mat, f)
        print('Saved sim_mat to {}'.format(config.LocalDirs.cache))

    # ----------------------------------------------------- vocab

    @staticmethod
    def make_vocab(w2cf, size, cat):
        print('Making vocab...')
        vocab = set()
        num_too_big = 0
        for text, f in sorted(w2cf.items(), key=lambda i: i[1], reverse=True):

            if len(text) > config.Corpus.max_word_size:
                num_too_big += 1
                continue

            doc = nlp(text, disable=['parser', 'ner'])
            w = doc[0]
            if cat != config.Corpus.no_cat:
                if w.pos_ == cat:
                    vocab.add(w.text.lower())
            else:
                vocab.add(w.text.lower())

            if len(vocab) == size:
                break

        print('Final vocab size={}'.format(len(vocab)))
        print('Excluded {} words that had more than {} characters'.format(num_too_big, config.Corpus.max_word_size))

        return list(vocab)

    @cached_property
    def cached_vocab_sizes(self):
        sizes = []
        for p in self.cache_path.glob('vocab_*.pkl'):
            size = int(regex_digit.search(p.name).group())
            sizes.append(size)
        return sorted(sizes)

    @cached_property
    def cached_vocab_names(self):
        names = []
        for p in self.cache_path.glob('vocab_*_{}.pkl'.format(config.Default.cat)):
            names.append(p.stem.replace('_', '-'))
        return sorted(names)

    def load_largest_vocab(self):
        largest_size = self.cached_vocab_sizes[-1]
        file_name = self.vocab_file_name_template.format(largest_size, config.Default.cat)
        with (self.cache_path / file_name).open('rb') as f:
            res = pickle.load(f)
        assert len(res) == largest_size
        return res

    @cached_property
    def vocab(self):
        if not self.cached_vocab_sizes:
            raise WikiNeighborsNoVocabFound(self.name)
        else:
            return self.load_largest_vocab()

    # --------------------------------------------------- sim mat

    @staticmethod
    def make_sim_mat(w2dfs, vocab, init_mat):  # TODO this may benefit from multiprocessing
        shape = (len(vocab), config.Max.num_docs)
        print('Making term-by-doc matrix with shape={}...'.format(shape))
        for col_id, w2df in enumerate(w2dfs):
            try:
                init_mat[:, col_id] = [w2df[w] for w in vocab]
            except IndexError:
                print(col_id)
        print('Done')
        return cosine_similarity(init_mat)

    def load_largest_sim_mat(self):
        largest_size = self.cached_vocab_sizes[-1]
        file_name = self.sim_mat_file_name_template.format(largest_size, config.Default.cat)
        with (self.cache_path / file_name).open('rb') as f:
            res = pickle.load(f)
        assert len(res) == largest_size
        return res

    @cached_property
    def sim_mat(self):
        if not self.cached_vocab_sizes:
            raise WikiNeighborsNoVocabFound(self.name)
        else:
            return self.load_largest_sim_mat()