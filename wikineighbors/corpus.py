from cached_property import cached_property
import numpy as np
from collections import Counter
from sklearn.metrics.pairwise import cosine_similarity
from multiprocessing import Pool
from timeit import default_timer as timer
import pickle
from wikineighbors.utils import regex_digit

from wikineighbors.params import Params
from wikineighbors.exceptions import WikiNeighborsNoArticlesFound
from wikineighbors.exceptions import WikiNeighborsNoVocabFound
from wikineighbors.utils import gen_100_param_names, to_param_path
from wikineighbors import config



class Corpus:
    """
    an abstraction.
    contains info about text file locations on shared drive
    """

    def __init__(self, name):
        self.name = name
        self.cache_path = config.LocalDirs.cache / name
        self.file_name_template = 'vocab_{}.pkl'

    # ------------------------------------------------ basic info

    @cached_property
    def num_param_names(self):
        return len(self.param_names)

    @cached_property
    def param_names(self):
        param_names = []
        parts = set()
        for param_name in gen_100_param_names(self.name):
            param_path = to_param_path(param_name)

            # make sure path exists
            if not param_path.exists():
                break  # this break is required, else infinite loop

            # make sure that param_name is valid (that it really is part of the corpus)
            params = Params(param_path)
            if params.part in parts:
                continue  # this happens because param_names are generated by returning all 8 possible param_names

            # collect
            parts.add(params.part)
            param_names.append(param_name)
        return param_names

    @cached_property
    def txt_paths(self):
        res = []
        for param_name in self.param_names:
            param_path = to_param_path(param_name)
            # check that articles are available (bodies.txt files)
            body_paths = [p for p in param_path.glob('**/bodies.txt')]
            num_bodies = len(body_paths)
            if num_bodies == 0:
                raise WikiNeighborsNoArticlesFound(param_path)
            elif num_bodies > 1:
                raise SystemError('Found more than 1 bodies.txt files in {}'.format(param_name))
            else:
                res.append(body_paths[0])

        return res

    # --------------------------------------------------- counting words

    @staticmethod
    def make_doc_vectors(p, stop_num, vocab):
        print('Starting worker')
        res = []
        with p.open('r') as f:
            for n, doc in enumerate(f):  # lazy generator
                words = doc.split()
                w2df = Counter(words)

                doc_vector = [w2df[w] for w in vocab]
                res.append(doc_vector)

                if n == stop_num:
                    print('Stopping worker after:', n)
                    return res

    def make_mat_with_cached_vocab(self):
        print('Counting words in documents...')
        start = timer()

        txt_paths = self.txt_paths[:]  # small benefit for multiprocessing: 16 vs. 28 seconds

        num_workers = len(txt_paths)
        vocab = self.vocab
        num_docs_per_worker = config.Max.num_docs // num_workers
        pool = Pool(num_workers)
        chunks = pool.starmap(self.make_doc_vectors,
                              [(p, num_docs_per_worker, vocab)
                               for p in txt_paths])
        pool.close()
        pool.join()

        print('Took {} secs to make document vectors '.format(
            timer() - start))

        # chunks is a list of chunks,
        # where each chunk is a list, that contains doc vectors (lists also)
        mat = np.concatenate(chunks).T

        print('Took {} secs to make term-by-window matrix of size {}'.format(
            timer() - start, mat.shape))
        return mat

    def make_w2cf(self):
        print('Counting all words in {} docs...'.format(config.Max.num_docs))
        start = timer()
        res = Counter()
        n = 0
        for p in self.txt_paths:
            with p.open('r') as f:  # it takes 8 seconds no matter how many processes due to network read
                for doc in f:  # lazy generator
                    words = doc.split()
                    w2df = Counter(words)
                    res.update(w2df)  # frequencies are incremented rather than replaced
                    if n == config.Max.num_docs:
                        break
                    n += 1

        print('Took {} secs to count all words in {} docs in 1 process'.format(
            timer() - start, config.Max.num_docs))
        return res

    # ------------------------------------------------------------ neighbors

    @cached_property
    def w2id(self):
        return {w: i for i, w in enumerate(self.vocab)}

    @staticmethod
    def to_sim_mat(mat):
        print('Making similarity matrix...')
        return cosine_similarity(mat)

    def get_neighbors(self, word, sim_mat):
        if not self.cached_vocab_sizes:
            raise WikiNeighborsNoVocabFound(self.name)

        print('Computing neighbors...')
        sims = sim_mat[self.w2id[word]]
        neighbors = [n for n in sorted(self.vocab, key=lambda w: sims[self.w2id[w]])
                     if n != word]
        return neighbors

    # --------------------------------------------------- i/o

    def save_vocab_to_disk(self, size):
        vocab = self.make_vocab(size)

        if not self.cache_path.is_dir():
            self.cache_path.mkdir(parents=True)
        with (self.cache_path / self.file_name_template.format(size)).open('wb') as f:
            pickle.dump(vocab, f)
        print('Saved vocab to {}'.format(config.LocalDirs.cache))

    # ----------------------------------------------------- vocab

    def make_vocab(self, size):
        print('Making vocab...')
        w2cf = self.make_w2cf()
        return [w for w, f in sorted(w2cf.most_common(size))]

    @cached_property
    def cached_vocab_sizes(self):
        sizes = []
        for p in self.cache_path.glob('*.pkl'):
            size = int(regex_digit.search(p.name).group())
            sizes.append(size)
        return sorted(sizes)

    def load_largest_vocab(self):
        largest_size = self.cached_vocab_sizes[-1]
        file_name = self.file_name_template = 'vocab_{}.pkl'.format(largest_size)
        with (self.cache_path / file_name).open('rb') as f:
            res = pickle.load(f)
        assert len(res) == largest_size
        return res

    @cached_property
    def vocab(self):
        if not self.cached_vocab_sizes:
            raise WikiNeighborsNoVocabFound(self.name)
        else:
            return self.load_largest_vocab()